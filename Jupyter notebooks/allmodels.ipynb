{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9050620682992803"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logistic regression\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv('C:/Users/nomesh.palakaluri.EMEA/OneDrive - Drilling Info/Desktop/Text model/dataset2/Data3.csv')\n",
    "sentences = df['word'].values\n",
    "y=df['label'].values\n",
    "#training and testing values\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.50, random_state=42)\n",
    "\n",
    "#converting the text data to numerical form\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train.astype('U'))\n",
    "X_train = vectorizer.transform(sentences_train.astype('U'))\n",
    "X_test  = vectorizer.transform(sentences_test.astype('U'))\n",
    "\n",
    "#specifying the model and fitting the data\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.44553929382215\n"
     ]
    }
   ],
   "source": [
    "#decision tree\n",
    "#decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree = dtree.fit(X_train, y_train)\n",
    "result=dtree.score(X_test,y_test)\n",
    "print(result*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.05187653963193\n"
     ]
    }
   ],
   "source": [
    "#random forest classifier\n",
    "# Import the model we are using\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, y, test_size=0.50, random_state=42)\n",
    "#converting the text data to numerical form\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train.astype('U'))\n",
    "X_train = vectorizer.transform(sentences_train.astype('U'))\n",
    "X_test  = vectorizer.transform(sentences_test.astype('U'))\n",
    "# Instantiate model with 1000 decision trees\n",
    "\n",
    "# Train the model on training data\n",
    "filename='C:/Users/nomesh.palakaluri.EMEA/OneDrive - Drilling Info/Desktop/Text model/finalized_model[random].sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model.fit(X_train,y_train)\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "classifier1=KMeans(3)\n",
    "filename = 'finalized_model[k-means].sav'\n",
    "pickle.dump(classifier1, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36721.82786433528\n"
     ]
    }
   ],
   "source": [
    "filename='C:/Users/nomesh.palakaluri.EMEA/OneDrive - Drilling Info/Desktop/Text model/finalized_model[k-means].sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model.fit(X_train)\n",
    "result = loaded_model.inertia_\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive_bayes = GaussianNB()\n",
    "filename = 'finalized_model[naive-bayes].sav'\n",
    "pickle.dump(naive_bayes, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.3185528667343\n"
     ]
    }
   ],
   "source": [
    "filename='C:/Users/nomesh.palakaluri.EMEA/OneDrive - Drilling Info/Desktop/Text model/finalized_model[naive-bayes].sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model.fit(X_train.todense(),y_train)\n",
    "result1=loaded_model.score(X_test.todense(),y_test)\n",
    "result = loaded_model.predict(X_test.todense())\n",
    "print(result1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "naive_bayes1 = MultinomialNB()\n",
    "filename = 'finalized_model[naive-bayes-multi].sav'\n",
    "pickle.dump(naive_bayes1, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.06665700623098\n"
     ]
    }
   ],
   "source": [
    "filename='C:/Users/nomesh.palakaluri.EMEA/OneDrive - Drilling Info/Desktop/Text model/finalized_model[naive-bayes-multi].sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model.fit(X_train.todense(),y_train)\n",
    "result1=loaded_model.score(X_test.todense(),y_test)\n",
    "result = loaded_model.predict(X_test.todense())\n",
    "print(result1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "naive_bayes2 = MultinomialNB()\n",
    "filename = 'finalized_model[naive-bayes-ber].sav'\n",
    "pickle.dump(naive_bayes2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.45065932473555\n"
     ]
    }
   ],
   "source": [
    "filename='C:/Users/nomesh.palakaluri.EMEA/OneDrive - Drilling Info/Desktop/Text model/finalized_model[naive-bayes-ber].sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model.fit(X_train.todense(),y_train)\n",
    "result1=loaded_model.score(X_test.todense(),y_test)\n",
    "result = loaded_model.predict(X_test.todense())\n",
    "print(result1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support vector machines classifier\n",
    "from sklearn import svm\n",
    "sv=svm.SVC() \n",
    "filename = 'finalized_model[svm].sav'\n",
    "pickle.dump(sv, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.44553929382215\n"
     ]
    }
   ],
   "source": [
    "filename='C:/Users/nomesh.palakaluri.EMEA/OneDrive - Drilling Info/Desktop/Text model/finalized_model[svm].sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model.fit(X_train,y_train)\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k nearest neigbours\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "filename = 'finalized_model[knn].sav'\n",
    "pickle.dump(knn, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.4334637492151\n"
     ]
    }
   ],
   "source": [
    "filename='C:/Users/nomesh.palakaluri.EMEA/OneDrive - Drilling Info/Desktop/Text model/finalized_model[knn].sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model.fit(X_train,y_train)\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=10, micro=2, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "RMDL: Random Multimodel Deep Learning for Classification\n",
    " * Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n",
    " * Last Update: 04/25/2018\n",
    " * This file is part of  RMDL project, University of Virginia.\n",
    " * Free to use, change, share and distribute source code of RMDL\n",
    " * Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n",
    " * Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n",
    " * Comments and Error: email: kk7nc@virginia.edu\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, tarfile\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "if sys.version_info >= (3, 0, 0):\n",
    "    import urllib.request as urllib  # ugly but works\n",
    "else:\n",
    "    import urllib\n",
    "\n",
    "print(sys.version_info)\n",
    "\n",
    "# image shape\n",
    "\n",
    "\n",
    "# path to the directory with the data\n",
    "DATA_DIR = '.\\Glove'\n",
    "\n",
    "# url of the binary data\n",
    "\n",
    "\n",
    "\n",
    "# path to the binary train file with image data\n",
    "\n",
    "\n",
    "def download_and_extract(data='Wikipedia'):\n",
    "    \"\"\"\n",
    "    Download and extract the GloVe\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    if data=='Wikipedia':\n",
    "        DATA_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "    elif data=='Common_Crawl_840B':\n",
    "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip'\n",
    "    elif data=='Common_Crawl_42B':\n",
    "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip'\n",
    "    elif data=='Twitter':\n",
    "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip'\n",
    "    else:\n",
    "        print(\"prameter should be Twitter, Common_Crawl_42B, Common_Crawl_840B, or Wikipedia\")\n",
    "        exit(0)\n",
    "\n",
    "\n",
    "    dest_directory = DATA_DIR\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    print(filepath)\n",
    "\n",
    "    path = os.path.abspath(dest_directory)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
    "                                                          float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath)#, reporthook=_progress)\n",
    "\n",
    "\n",
    "        zip_ref = zipfile.ZipFile(filepath, 'r')\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "        zip_ref.close()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NOUN       0.69      1.00      0.81      8066\n",
      "        VERB       0.96      0.51      0.67       768\n",
      "           o       1.00      0.58      0.73      7729\n",
      "\n",
      "    accuracy                           0.78     16563\n",
      "   macro avg       0.88      0.70      0.74     16563\n",
      "weighted avg       0.85      0.78      0.77     16563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rocchio classification\n",
    "\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = df.word.fillna(' '), df.label\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y, test_size = 0.2, random_state = 100) \n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', NearestCentroid()),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NOUN       0.70      1.00      0.82      8066\n",
      "        VERB       0.96      0.35      0.51       768\n",
      "           o       1.00      0.61      0.76      7729\n",
      "\n",
      "    accuracy                           0.79     16563\n",
      "   macro avg       0.89      0.65      0.70     16563\n",
      "weighted avg       0.85      0.79      0.78     16563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#boosting and bagging\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "X, y = df.word.fillna(' '), df.label\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y, test_size = 0.2, random_state = 100) \n",
    "\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', GradientBoostingClassifier(n_estimators=100)),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NOUN       1.00      0.80      0.89      8066\n",
      "        VERB       0.96      0.72      0.83       768\n",
      "           o       0.81      1.00      0.90      7729\n",
      "\n",
      "    accuracy                           0.89     16563\n",
      "   macro avg       0.92      0.84      0.87     16563\n",
      "weighted avg       0.91      0.89      0.89     16563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "X, y = df.word.fillna(' '), df.label\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y, test_size = 0.2, random_state = 100) \n",
    "\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', BaggingClassifier(KNeighborsClassifier())),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ac7df784fb9f66341270b4235b901a57e4c94a02ff00519a90c3611eb6bf397"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
